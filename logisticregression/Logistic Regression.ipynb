{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Theano\n",
    "\n",
    "Logistic regression is a method for learning a mapping between data and labels. Here we'll focus on the problem where each data sample has exactly one label. The data will be images of handwritten digits, the MNIST dataset, between 0 and 9.\n",
    "\n",
    "The main goals for this will be to:\n",
    " * get familiar with building machine learning models in Theano,\n",
    " * learn about optimization/learning techniques, and\n",
    " * practice working with a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You'll need a few different libraries to get started\n",
    "\n",
    "# Common python science libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Libraries for this project\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.compat.python2x import OrderedDict\n",
    "from pylearn2.datasets import mnist\n",
    "from utils import tile_raster_images as tri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "We'll use Pylearn2's built-in MNIST dataset object to load the data for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = mnist.MNIST('train')\n",
    "X = ds.X\n",
    "y = ds.y\n",
    "print('Initial data shape: {}'.format(X.shape))\n",
    "dim = X.shape[1]\n",
    "classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a few images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "side = int(np.sqrt(n))\n",
    "im = tri(X[:n], (28, 28), (side, side), (2, 2))\n",
    "plt.imshow(im, cmap='gray', interpolation='nearest')\n",
    "plt.show()\n",
    "for ii in range(side):\n",
    "    print(y[ii*side:(ii+1)*side].ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "Theano is a bit of an odd library if you've ever worked with numpy or matlab.\n",
    "\n",
    "You first setup the symbolic computation you want to do. Then, you feed data into this computation to get the results you are interested. Normally, these two steps are mixed together.\n",
    "\n",
    "### Symbolic computation\n",
    "To setup the symbolic computation we'll need to define the inputs to our computation. They'll come in two variants: inputs that just get used once and inputs that we'll want to keep track of and update.\n",
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First define inputs that get used once\n",
    "X_sym = T.matrix('X')\n",
    "y_sym = T.lvector('y')\n",
    "\n",
    "# Then define the parameters which will be kept and updated (called shared variables)\n",
    "w_0 = np.random.randn(28**2, 10).astype('float32')\n",
    "W = theano.shared(w_0)\n",
    "b = theano.shared(np.zeros(10).astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the network predictions\n",
    "# T.nnet.softmax gives multinomial classification\n",
    "y_hat = \n",
    "\n",
    "# Calculate cost\n",
    "# T.nnet.categorical_crossentropy is right for classification\n",
    "cost = \n",
    "# Accuracy can be computer with T.argmax\n",
    "accuracy = \n",
    "\n",
    "W_grad = T.grad(cost, W)\n",
    "b_grad = T.grad(cost, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "We now have to choose how we want to update W and b. Gradient descent is the easiest option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eps = .001\n",
    "updates = OrderedDict()\n",
    "W_prime = W - eps * W_grad\n",
    "b_prime = b - eps * b_grad\n",
    "updates[W] = W_prime\n",
    "updates[b] = b_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Theano function\n",
    "The \"Theano function\" is the thing you'll feed your data into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = theano.function(inputs=[X_sym, y_sym], outputs=[cost, accuracy])\n",
    "f_updates = theano.function(inputs=[X_sym, y_sym], outputs=[cost, accuracy], updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Now that we have the symbolic computation setup, we need to feed a bunch of data in to learn W and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_batches = 100\n",
    "\n",
    "for ii in range(n_batches):\n",
    "    idx = np.random.randint(low=0, high=X.shape[0]-batch_size)\n",
    "    X_batch = X[idx:idx+batch_size]\n",
    "    y_batch = y[idx:idx+batch_size]\n",
    "    c, a = f_updates(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does the network learn?\n",
    "We can look at W and see what the network learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im = tri(w_0.T, (28, 28), (1, 10), (2, 2))\n",
    "plt.imshow(im, cmap='gray', interpolation='nearest')\n",
    "plt.title('Original weights')\n",
    "plt.figure()\n",
    "im = tri(W.get_value().T, (28, 28), (1, 10), (2, 2))\n",
    "plt.imshow(im, cmap='gray', interpolation='nearest')\n",
    "plt.title('Learned weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projects\n",
    " * How does the test set cost and accuracy compare to the training set? Test set can be loaded with ts = ds.get_test_set()\n",
    " * How does the training change if you change eps?\n",
    " * Can you train with momentum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
