{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io.wavfile as wavfile\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Timeseries Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll spend an embarrassing amount of time as a young scientist loading, processing, and saving data that streams in over time. In this tutorial you'll practice loading a few .wav audio files and plotting them as waveforms, power spectra, and spectrograms.\n",
    "\n",
    "This folder should contain two wav audio files. They are a sentence from *Gulliver's Travels* being read by a male and female speaker. We'll see if we can distinguish them based on analysis of the audio data. It is probably very easy to hear which is which by listening directly to the sound files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use functions in *os* to find files ending in \".wav\" from the current folder and wavfile to load the into the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = os.listdir('.')\n",
    "files = [f for f in files if '.wav' in f]\n",
    "wav_data = [wavfile.read(f) for f in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect *wav_data* in the following cell to understand what information is returned. You can also look at the documentation by googling \"scipy.io.wavfile\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot both waveforms to see what they look like as a function of time. Keep in mind the scale may be different for the two graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot()# Plot the first one here\n",
    "plt.title('First speaker')\n",
    "plt.figure()\n",
    "plt.plot()# Plot the second one here\n",
    "plt.title('Second speaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you tell which speaker is male or female from this plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Power Spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Power Spectrum of a signal is defined as the square of the Fourier Series Coefficients of the signal. It measures the amplitude-squared of each frequency in the signal. Let's calculate the plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampling_rate, signal_1 = wav_data[0]\n",
    "sampling_rate, signal_2 = wav_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fft_1 = np.fft.fftshift(np.fft.fft(signal_1))\n",
    "freq_1 = np.fft.fftshift(np.fft.fftfreq(signal_1.shape[0], 1./sampling_rate))\n",
    "fft_2 = np.fft.fftshift(np.fft.fft(signal_2))\n",
    "freq_2 = np.fft.fftshift(np.fft.fftfreq(signal_2.shape[0], 1./sampling_rate))\n",
    "\n",
    "pow_1 = #Get the square of the magnitude of the fft\n",
    "pow_2 = #Get the square of the magnitude of the fft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using google, figure out what the functions *fftshift* and *fftfreq* do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(freq_1, pow_1)\n",
    "plt.title('Power Spectrum for Signal 1')\n",
    "plt.figure()\n",
    "plt.plot(freq_2, pow_2)\n",
    "plt.title('Power Spectrum for Signal 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's Probably difficult to visually parse. Copy the code above and plot the Log-Power Spectrum (use np.log on the *pow_x* variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the log-power spectrum here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you tell from this plot which speaker is male and which is female?\n",
    "\n",
    "One downside of the power spectrum is that we have lost all time information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrogram Format--Sliding FFT Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, for sound data, we'd like a format that has both time and frequency information. One simple way of getting this data is to take the spectrum for small windows and move the window through the file. You'll need to choose a window length in milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "window_ms = 100.# Window length in milliseconds\n",
    "spects = []\n",
    "freqs = []\n",
    "for sampling_rate, signal in wav_data:\n",
    "    window_pts = int(sampling_rate*window_ms/1000.)\n",
    "    n_windows = int(signal.shape[0]/window_pts)\n",
    "    temp_spect = np.empty(shape=(n_windows, int(window_pts/2)+1))\n",
    "    for ii in xrange(n_windows):\n",
    "        fft = np.fft.fftshift(np.fft.fft(sp.kaiser(window_pts, 7)*signal[ii*window_pts:(ii+1)*window_pts]))\n",
    "        pow_spect = np.square(np.absolute(fft))\n",
    "        temp_spect[ii] = pow_spect[:int(window_pts/2)+1]\n",
    "    freqs.append(np.fft.fftshift(np.fft.fftfreq(window_pts, 1./sampling_rate))[int(window_pts/2)+1:])\n",
    "    spects.append(temp_spect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.log(spects[0].T+1.e-10), aspect='auto', \n",
    "           interpolation='nearest', extent=[0, int(window_ms*len(spects[0])/1000.), freqs[0].min(), freqs[0].max()])\n",
    "plt.figure()\n",
    "plt.imshow(np.log(spects[1].T+1.e-10), aspect='auto', \n",
    "           interpolation='nearest', extent=[0, int(window_ms*len(spects[1])/1000.), freqs[1].min(), freqs[1].max()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the y-axis on a log scale can make it easier to see what is happening at different frequencies. You'll need to add \"plt.yscale('log')\" after each imshow. Can you tell which speaker is male or female now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
